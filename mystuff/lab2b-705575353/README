NAME: Nevin Liang
EMAIL: nliang868@g.ucla.edu
ID: 705575353

FILE DESCRIPTION
=================

lab2_list.c:    a C program that implements the (below) specified command line options 
                and produces the (below) specified output statistics

SortedList.h:   a header file (supplied by us) describing the interfaces for linked 
                list operations.

SortedList.c:   the source for a C source module that compiles cleanly (with no errors or warnings), 
                and implements insert, delete, lookup, and length methods for a sorted doubly linked 
                list (described in the provided header file, including correct placement of 
                pthread_yield calls).

Makefile:       make lab2a: the lab2_list executable (compiling with the -Wall and -Wextra options).
                make profile: run tests with profiling tools to generate an execution profiling report
                make tests: run all specified test cases to generate CSV results
                make graphs: use gnuplot to generate the required graphs
                make dist: create the deliverable tarball
                make clean: delete all programs and output generated by the Makefile

listtest.sh:    list of all tests as a shell script. run by the make file.

*.png:          created by gnuplot(1) on the above csv data. descriptions at top of .gp file

profile.out:    execution profiling report showing where time was spent in the un-partitioned 
                spin-lock implementation.

lab2_list.gp:   gnuplot file that generates all graphs using grep of csv file data

lab2_list.csv:  containing your results for all of test runs

QUESTIONS
==========
Question 2.3.1: Most of the cycles for the 1-2 thread list tests are probably being spent in the
                list operations. Especially if the number of iterations is huge, i.e. thousands,
                the time that locks take up (the other possibility) is not that much because the #
                of threads is very low. On the other hand, running through the entire list and
                finding where to insert or remove takes a long time. As for the high thread spin-lock
                tests, most of the CPU time lies on waiting for the spin locks to stop spinning. For
                mutex locks, I would say that most of the TIME is waiting for the locks to unlock, but
                that isn't really CPU time because the threads are blocked. Still, however, throughput is
                pretty low for mutex locks at high thread count.
Question 2.3.2: The lock function (which was called by my thread controller function) was where most of the
                time was being spent. It took up a large part as per my profile.out file said, at least 80
                percent. If there are many many threads (not 1), then each thread when waiting will have 
                to spend time in that function.
Question 2.3.3: The more threads there are, the more time each thread must wait for all the other threads 
                to complete in order for it to get its turn. (<< not technically ALL threads but more
                threads if # of total threads is higher, but thats the gist). The time for completion depends
                entirely on the number of iterations and not the number of threads, so when threads increases
                the only extra time per operation is because multiple threads need to run operations on the same
                list. However, wait time rises more dramatically than time per operations because wait time
                depends on the number of threads while the time per ops does not
Question 2.3.4: The more lists there are, the higher the throughput and the faster the script completes.
                This is because multiple threads can run at the same time on different lists and don't have
                to wait for each other to unlock because there is no critical section that they share. they
                are all running on separate lists. It should continue to increase however little, until threads
                barely compete over locks because each element has its own list. I dont think so. With more lists,
                the length of each list is reduced, as the total length only equals threads * iterations. Now, it is
                threads * iterations / lists. The throughput of a partioned list does not only have to do with 
                threads and lists. it has to do with iterations as well.


SOURCES USED
=============
mostly the TA powerpoint slides
Man Pages:
    - https://www.gnu.org/software/make/manual/
    - https://www.man7.org/linux/man-pages/man1/chmod.1.html
    - https://linux.die.net/man/3/pthread_create
    - https://www.man7.org/linux/man-pages/man3/pthread_join.3.html
    - https://linux.die.net/man/3/malloc
    - https://linux.die.net/man/3/clock_gettime
    - https://linux.die.net/man/3/strcat
    - https://www.ibm.com/support/knowledgecenter/SSGH2K_13.1.3/com.ibm.xlc1313.aix.doc/compiler_ref/bif_gcc_atomic_val_comp_swap.html
    - https://www.ibm.com/support/knowledgecenter/en/SSGH2K_13.1.0/com.ibm.xlc131.aix.doc/compiler_ref/bif_gcc_atomic_lock_release.html
    - https://github.com/gperftools/gperftools
    - https://man7.org/linux/man-pages/man1/gprof.1.html
